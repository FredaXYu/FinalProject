gcn achieve good performance many graph task poor scalability gcn transductive learn method manet generation new node change link make difficult extend gcn network unknown topological structure use gcn train network high density neighbor extension exceedingly small number node contain large portion full graph noticeably short time bring huge computation cost fact ordinary gcn satisfy application fastchanging manet section introduce detail link prediction model manet name fastslsg propose paper framework faststlsg show fig 2 mainly divide three unit 1 time slice 2 generator include fastgcn gru desne layer 3 discriminator follow describe motivation adopt unit specifically time slice unit place input model convert manet topological data series continuous static snapshot reasonable timestamp compress data reduce data redundancy improve ability information extraction base snapshot interval estimate duration link future generator fastgcn extract spatial feature static snapshot fee result network embed gru extract network temporal feature gain continuous evolution law manet dense layer use decoder transform extract feature back original space generate prediction result consider sparsity network link node pair network far smaller nonlinked node pair negative impact recovery topology use lsgans improve generalization generation ability model process model train use observe network structure input discriminator guide generator generate highquality prediction result addition adoption lsgans also due use fastgcn base node sample fastgcn greatly improve train speed lead loss information node therefore use lsgans improve performance model follow describe unit separately detail time slice unit draw analysis method time series data divide original continuously change manet topological structure series static snapshot time slice use input data faststlsg spatial temporal feature extraction interval consecutive slice number network snapshot slice time element adjacency matrix snapshot link weight represent link duration node u v previous timestamp next timestamp partition manet topology c discrete static weight snapshot use slice time t. obviously value directly affect accuracy model short input data highly correlate model tend insensitive relatively independent new feature prediction result bias data derive redundant feature long input data contain many new feature difficult model extract effective feature large number new feature result low prediction accuracy paper dynamical behavior node manet regard chaotic system autocorrelation function method use determine reasonable slice time length borrow coordinate delay phase space reconstruction technique chaotic time series theory correlation network snapshot slice time c represent number static snapshot adjacency matrix ith snapshot mean value element correspond adjacency matrix selflearning model lower correlation input data higher independence data feature relate study show usually appropriate determine value drop first time practical application basis select optimal slice time paper determine appropriate manet transform c weight network snapshot use time window length move smoothly snapshot sequence obtain series consecutive snapshot set length s. far subsequent train task faststlsg model learn function map input sequence give sequence snapshot length s. section gcn introduce basic idea gcn design subtle way extract feature graph data obtain embed representation network consider drawback poor scalability high complexity neighbor computation gcn adopt fastgcn spatial feature extraction network snapshot fastgcn node snapshot consider independent identically distribute sample base probability distribution convolution operation loss function gcn transform integral calculation embed function base certain probability measure graph convolution operation loss function form integral approximate use monte carlo method thus node select batch model train like inductive learn structure graph separate faststlsg train predict connection state node change effectively improve generalization ability scalability model manet addition compare embed method gcn graphsage fastgcn reduce time complexity improve efficiency algorithm use monte carlo method approximate computation convolution loss function node sample fastgcn simplest way sample node use uniform distribution sample also make select node close real distribution importance sample reduce error cause uniform sample improve performance model summary fastgcn effectively solve defect ordinary gcn spatial feature extraction largescale fastchanging manet make propose model faststlsg suitable practical application slice network obtain snapshot set snapshot feed fastgcn unit spatial feature extraction graph convolution operation fastgcn describe exist static snapshot g weight matrix z snapshot sequence v node g. convolution operation g fastgcn regard integral calculation embed function node v node upper layer show eq 3. v u node snapshot treat independent random variable probability measure element adjacency matrix set contain parameter train lth layer embed result node u lth layer calculate integral transformation embed function node upper layer particular data input layer representation correspond node characteristic matrix convolution operation graph express form integral function make eq 3 approximately calculate monte carlo method lth layer node sample independently uniformly probability p obtain sample node equation 3 express approximately uniformly sample node layer finally get node uniformly sample node lrow recursively represent figure 3 4 show comparison gcn fastgcn gcn spatial feature node obtain aggregation feature node upper layer computational complexity gcn fastgcn large graph divide several small graph batch node require sample convolution operation computational complexity fastgcn sparsity network train efficiency fastgcn greatly improve compare gcn improve ability spatial feature extraction use importance sample instead uniform sample fastgcn node sample probability distribution q effectively reduce sample variance make distribution sample node closer real network structure probability mass function pmf node network show eq 6. eq 6 see pmf depend parameter l i.e. sample distribution layer need update sample distribution function real time train proceed update eq 57 sample node accord distribution paper use two layer spatial feature extraction initial data characteristic matrix z represent link weight snapshot summary fastgcn unit extract spatial feature base adjacency matrix input manet snapshot correspond feature matrix output series network embed result obtain series embed result snapshot series capture longterm temporal correlation snapshot time sequence key issue predict future structure manet rnns effectively process time series data analyze temporal characteristic sequence data use temporal dependence historical data complete prediction current future moment input data long sequence upper layer rnn unable learn sequence feature gradient disappearance result rnn ability shortterm learn difficult use previous historical information handle later data sequence solve problem shortterm memory long shortterm memory lstm network elaborately design gate selectively change flow information historical sequence decide whether information historical sequence need retain discard keep important feature front variant lstm gru simpler structure fewer train parameter also avoid gradient disappearance retain longterm sequence information order improve train efficiency faststlsg better apply fastchanging manet gru use paper extract temporal feature network snapshot sequence faststlsg embed result fastgcn unit input gru unit sequentially capture dynamic evolution manet time sequence gru describe package module repeatedly combine multiple multiplication gate cell unit status update gate reset gate take time step example input gru unit input vector current moment state vector previous moment status gate gru show represent update gate reset gate respectively use control much information previous state bring current state larger value information previous state bring use control much information previous state write current candidate set smaller value le information previous state add parameter gru need train mean two vector concatenate mean product matrix choose gru basic unit fast convergence speed improve train speed gru need maintain three parameter correspond update gate reset gate candidate set respectively faststlsg output size hide size equal therefore complexity gru unit number gru gru unit n hide size m input size sum faststlsg input gru unit embed result historical network snapshot output hide layer last cell gru fee fully connect layer train generate predict manet structure next time paper employ lsgans improve ability feature extraction data generation generative adversarial network gin generative model receive much attention recent year achieve widespread success field computer vision image recognition natural language process core idea gin derive nash equilibrium game theory mainly compose generator g discriminator d. goal g try learn real data distribution generate fake data input real data fake data output probability value identify input real data want correctly distinguish whether input data real data g. meanwhile output feed back g guide g 's train ideal case model reach optimality unable distinguish source input data process train g update parameter minimize loss function continuous iterative optimization nash equilibrium state finally reach model optimal objective function gin define x input data z represent noise generate base probability distribution however standard gin problem gradient disappearance train process unstable lead unsatisfactory generation result reason problem although correct classification result obtain use crossentropy data classify true far away real sample use iterate anymore successfully cheat d. lead saturation state easily gradient dispersion g 's update lsgans use leastsquares loss replace crossentropy loss standard gin construct pearson divergence instead jensenshannon j divergence finally construct stable efficient powerful adversarial network different distance metric specific loss function train procedure lsgans describe section loss function faststlsg consider adversarial generator discriminator minimax game lsgans input g sequence historical network snapshot output predict future network structure us real future network structure condition discriminate whether generate prediction result come g train stable prediction result generate g deceive consider g high quality complete link prediction manet show fig 2 generator faststlsg compose fastgcn gru dense layer unit fastgcn unit extract spatial feature historical network snapshot input adjacency matrix feature matrix output embed result embed result transform vector input gru unit gru unit use extract temporal feature historical snapshot use powerful sequential data extraction ability output gru unit state vector hide layer vector last time stage input dense layer output dense layer prediction result manet time t. sum input output g simply express historical manet structure predict manet structure moment faststlsg discriminator use discriminate whether input prediction network generate g. consist dense layer activation function model train output g real network adjacency matrix alternately feed d. use input dense layer train output calculate activation function complete discrimination worth note input dense layer form vector dimensional matrix need transform vector feed dense layer summary input output simply express weight parameter bias parameter train dense layer output layer respectively computational complexity lsgans unit introduce computational complexity lsgans relate network size number element matrix process train g one unit fix unit 's parameter update alternate iteration loss function faststlsg train divide adversarial loss reconstruction loss describe adversarial loss loss function g adversarial process lsgans use least square loss function penalize sample discriminate true far away decision boundary drag false sample far away decision boundary decision boundary improve quality g. adversarial loss function express follow distribution snapshot represent snapshot represent snapshot represent prediction result manet constant b encode real network data topology generate g respectively c encode set treat network structure generate g real network objective function equivalent pearson divergence faststlsg finally adversarial loss faststlsg follow faststlsg g want prediction result close real result possible want discriminative power stronger adversarial loss function need minimize need prediction result close possible actual network improve accuracy prediction use mean square error mse measure similarity reconstruction loss follow however due sparsity network zero element adjacency matrix network much larger nonzero element lead g incline generate many zero element make loss function unable converge even fit solve problem sparsity network use penalty matrix p impose greater penalty nonzero element improve reconstruction loss follow hadamard product element satisfy otherwise set p impose penalty nonzero element guide generate zero element correspond position much possible also prevent overfitting introduce l2 regularization punish square term parameter impose greater penalty large weight represent matrix contain parameter train g coefficient control penalty effect l2 regular term summary combine eq 13 15 16 overall loss function faststlsg show eq 17. model train adam optimizer use alternately update parameter matrix g iteration terminate g reach equilibrium train complete historical manet topology input g obtain prediction manet future moment thus achieve link prediction manet pesudocode faststlsg propose paper show table 2. addition declare paper method carry accordance relevant guideline \<SEP>3