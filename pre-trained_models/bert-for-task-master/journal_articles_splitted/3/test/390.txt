classification preprocessed image sequence use improve shufflenet network load model weight pretraining classify image sequence sample equipment sample condition use build dataset also vary institution institution result large difference data resolution code rate metric dataset data different datasets process equally study schematic diagram preprocessing step show fig 2. utilize original expression image sequence provide dataset process obtain complete microexpression image sequence use opencv dlib toolkit extract face position facial marker point crop face size pixel face need align ensure always horizontal 68 facial landmark point detector provide dlib toolkit use obtain coordinate leave eye right eye calculate coordinate center two eye center rotation angle desire rotation obtain calculate tangent horizontal vertical distance leave eye center point face picture rotate horizontal affine transformation angle calculate eq 1. rotation horizontal optical flow feature extract onset apex frame final step eye region region produce little movement mask mark facial marker point avoid noise cause eye movement e.g. blink reduce redundant feature microexpression dataset contain sufficient data number expression different category extremely unbalance count sample could easily result bias neural network positive emotion well surprise emotion therefore expand horizontal inversion number emotion category essentially balance expansion movement microexpressions subtle last le 0.2 s. addition sample use study collect laboratory fix condition consequently microexpression sample satisfy condition use optical flow feature luminance channel value remain essentially constant pixel move little adjacent frame suppose light energy point first frame denote pixel coordinate time time point move distance intensity two adjacent frame achieve study xu et al perform taylor expansion eq 2 obtain divide side equation equally dt yield horizontal vertical component optical flow field respectively assume constant brightness 1 produce constraint equation sufficient solve u therefore solve optical flow field model require additional constraint displacement field vector propose add smooth constraint optical flow constraint assume velocity object motion locally smooth case particular target rigid motion without deformation velocity neighbor pixel point i.e. rate change velocity neighbor point zero solution optical flow field transform energy generalize minimal value problem first term regular term assume optical flow field show large variation smooth velocity field obtain second term data term basic optical flow constraint assume constant grayscale value correspond point motion regularization parameter weight parameter associate regular term data term since regular term h model adopt smoothness constraint regular term data term quadratic robustness poor keep discontinuity displacement field cause serious blur loss important information image evolution overcome shortcoming h model tvl1 optical flow model base full variational approach propose optical flow constraint improve introduce variable w. model light change obtain parameter factor weight light change term although little change compare h model alignment accuracy greatly improve first total variation regular term maintain discontinuity displacement field protect edge information blur diffusion replace regular term good denoising effect keep edge information blur secondly data item tvl1 le sensitive luminance change compare h model experiment robust tvl1 approach choose first part pretraining extract number sample category order prevent network model bias towards specific category emotion classification feature image feed network model randomly without repetition make perform classification process initial learn rate set 0.001 crossentropy loss function use calculate loss softmax classifier use classify image pretraining final accuracy 96.28 achieve identify optical flow feature image model weight save selfattentionbased vit method achieve excellent result many computer vision problem past year however model tend complex pair approach adopt paper order optimize accuracy microexpression recognition maintain lightweightness model form new network model convolutional block attention module add shufflenetv2 first approach result approach attention map add feature map cnn model structure nonnegligible timespace overhead addition incorporate improve lightweight transformer module shufflenetv2 alternative approach contain full operation transformer retain cnnspecific feature inductive bias global nature transformer overall model still remain lightweight accurate improve transformer block lightweight convolutional block attention module feedforward neural network propose 2018 infer attentional feature map two independent dimension channel space multiply attentional feature map input feature map refine feature adaptively view generality module seamlessly integrate cnn architecture minimal time space overhead spatial information feature map first aggregate use average pool maximum pool operation generate two different spatial context descriptor denote average pool feature maximum pool feature respectively two descriptor forward share network generate channel attention map share network consist multilayer perceptron mlp one hide layer reduce parameter overhead hide activation size set scale rate apply share network descriptor merge output feature vector use elementbyelement summation brief channel attention calculate follow denote sigmoid function note two input share mlp weight follow relu activation function spatial attention map generate exploit spatial interrelationship feature unlike channel attention spatial attention focus information component complement channel attention compute spatial attention first apply average pool maximum pool operation along channel ax concatenate generate valid feature descriptor apply pool operation along channel axis effective highlight area information connect feature descriptor convolutional layer apply generate spatial attention map encode emphasize suppress location two twodimensional map generate use two pool operation aggregate channel information feature map denote average pool feature maximum pool feature whole channel connect convolve standard convolutional layer generate 2d spatial attention map brief spatial attention calculate follow denote sigmoid function denote convolution operation filter size complete model structure show fig 3. various task include image classification target detection semantic segmentation significantly improve original vit cnns nevertheless performance improvement usually require high computational resource example deit multiadds 10 g computational resource perform image classification task high computational resource requirement beyond capability many device also difficult capture subtle facial change lightweight efficient convolutional neural network design mobile vision task due localization problem extract feature result hope combine vit shufflenetv2 complement one another improvement vit module mainly include way selfattention compute different image encode method ensure module contain local feature extraction capability similar convolutional layer speed train efficiency vit module first note underlie transformer consist alternate multiheaded selfattention multilayer perceptron mlp calculation selfattention express eq query key value matrix dk querykey channel dimension number token token channel dimension lightweight model limit capacity computational cost selfattentiveness higher convolutional layer computational complexity selfattention quadratic respect spatial resolution introduce three linear layer level compute linear combinatorial result alleviate problem receive inspiration introduce ghost module replace linear layer selfattention us ordinary convolution generate inherent feature map enhance feature add channel use linear operation le computational overhead obtain better performance speed performance secondly weight share mechanism utilize reuse weight calculation reuse feature due fact et al argue involve computation attentional map final result selfattention mechanism linear combination token compare need retain semantic information ensure final weight representational power result thus result selfattentive mechanism strongly correlate weakly correlate therefore simplify computation lighten overall computational overhead model weight share mechanism express follow projection calculate respectively improve calculation selfattentiveness write finally feed feature map transformer module standard convolutional layer first apply feature map feature generate use point convolution convolutional layer encode local spatial information point convolution project tensor highdimensional space learn linear combination input channel first standard convolutional layer apply give image input tensor width height number channel image respectively point convolutional layer connect yield convolutional layer encode local spatial information contrast point convolutional layer project tensor highdimensional space learn linear combination input channel method expand n nonoverlapping flatten block number block height width block respectively relationship block encode apply improve transformer obtain unlike vanilla vit lose spatial order pixel vit block lose neither block order spatial order pixel within block therefore collapse obtain project low cdimensional space use pointtopoint convolution combine join operation another convolution layer use fuse local global feature tandem tensor output since us convolution encode local information n ¡Á n region encode global information p block pth position pixel encode information pixel show fig 4. thus vit block loss effective sensory field model structure introduction improve vit module show fig 5. shufflenetv2 upgrade version propose face tsinghua university 2018 shufflenetv1 accurate shufflenetv1 mobilenetv2 complexity analyze shufflenetv1 researcher find pointbypoint group convolution bottleneck structure increase memory access costmac cost negligible elementlevel addition operation shortcut connection also result reduce efficiency shufflenetv2 introduce channel split show fig 6 divide input feature channel two branch one remain unchanged consist three convolution input output channel two convolution longer group convolution ensure minimal memory access cost module shufflenetv2 network use channel feature larger network capacity thus maintain high accuracy reduce computational complexity addition due channel segmentation part feature connect follow block directly equivalent feature reuse make network efficient also accurate network structure use paper show table 1. basic shufflenetv2 us softmax classifier softmax regression improvement base logistic regression multiclassification problem give sample input output value 0 1 indicate probability input sample belong class expression image probability class j probability expression image correspond expression category parameter fit category highest probability value final result neural network prediction classification number sample microexpression dataset small difference different emotion sample subject subtle since softmax classifier minimize crossentropy global perspective softmax classifier likely lead misclassification sample large difference solve problem svm perform well small data set often higher classification accuracy classifier also try use svm classifier classification svm try find maximum margin data point different category better differentiability regularization term penalize wrongly score data strongly strong generalization capability thus facilitate differentiation microexpression feature hinge loss function define 1 ¡Ü ¡Ü n denote label experiment l2 regularization add model add regularization allow model use many feature possible identify emotional feature rather individual feature besides replace relu activation function leakyrelu avoid occurrence neuron death \<SEP>3