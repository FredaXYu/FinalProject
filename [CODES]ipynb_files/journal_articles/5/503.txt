pablo contreras kallens receive fund new frontier grant program cornell university unlike carefully script dialogue find book movie language everyday interaction tend messy incomplete full false start interruption people talk casual conversation friend bicker sibling formal discussion boardroom authentic conversation chaotic seem miraculous anyone learn language give haphazard nature linguistic experience reason many language scientist include noam chomsky founder modern linguistics believe language learner require kind glue rein unruly nature everyday language glue grammar system rule generate grammatical sentence child must grammar template wire brain help overcome limitation language experience think go template example might contain superrule dictate new piece add exist phrase child need learn whether native language one like english verb go object eat sushi one like japanese verb go object japanese sentence structure sushi eat new insight language learn come unlikely source artificial intelligence new breed large ai language model write newspaper article poetry computer code answer question truthfully expose vast amount language input even astonishingly without help grammar grammatical language without grammar even choice word sometimes strange nonsensical contain racist sexist harmful bias one thing clear overwhelm majority output ai language model grammatically correct yet grammar template rule hardwired rely linguistic experience alone messy may gpt3 arguably wellknown model gigantic deeplearning neural network 175 billion parameter train predict next word sentence give come across hundred billion word internet book wikipedia make wrong prediction parameter adjust use automatic learn algorithm remarkably gpt3 generate believable text react prompt summary last 'fast furious movie ... write poem style emily dickinson moreover gpt3 respond sit level analogy read comprehension question even solve simple arithmetic problem learn predict next word compare ai model human brain similarity human language stop however research publish nature neuroscience demonstrate artificial deeplearning network seem use computational principle human brain research group lead neuroscientist uri hasson first compare well gpt2 little brother gpt3 human could predict next word story take podcast american life people ai predict exact word nearly 50 time researcher record volunteer brain activity listen story best explanation pattern activation observe people 's brain like gpt2 use precede one two word make prediction rely accumulate context 100 previous word altogether author conclude find spontaneous predictive neural signal participant listen natural speech suggest active prediction may underlie human lifelong language learn possible concern new ai language model feed lot input gpt3 train linguistic experience equivalent 20,000 human year preliminary study yet peerreviewed find gpt2 still model human nextword prediction brain activation even train 100 million word well within amount linguistic input average child might hear first 10 year life suggest gpt3 gpt2 learn language exactly like child indeed ai model appear comprehend much anything say whereas understand fundamental human language use still model prove learner albeit silicon one learn language well enough mere exposure produce perfectly good grammatical sentence way resemble human brain process rethink language learn year many linguist believe learn language impossible without builtin grammar template new ai model prove otherwise demonstrate ability produce grammatical language learn linguistic experience alone likewise suggest child need innate grammar learn language child see hear go old say latest ai language model suggest nothing could truth instead child need engage backandforth conversation much possible help develop language skill linguistic experience grammar key become competent language